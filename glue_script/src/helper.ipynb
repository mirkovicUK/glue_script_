{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d003ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import boto3\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11049b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_from_s3(bucket_name, s3, logger=None):\n",
    "    \"\"\"Retrieve a list of files from an S3 bucket.\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        s3 (boto3.client): The S3 client to use for accessing the bucket.\n",
    "        logger (Logger, optional): Logger for logging messages. Defaults to None.\n",
    "    Returns:\n",
    "        list: List of file names in the S3 bucket.\"\"\"\n",
    "    try:\n",
    "        # List objects in the specified S3 bucket\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.error(f\"Error accessing bucket {bucket_name}: {e}\")\n",
    "        else:\n",
    "            # If no logger is provided, print the error\n",
    "            print(f\"Error accessing bucket {bucket_name}: {e}\")\n",
    "        return []\n",
    "    files = []\n",
    "    if 'Contents' in response:\n",
    "        for item in response['Contents']:\n",
    "            files.append(item['Key'])\n",
    "    return files\n",
    "\n",
    "def partition_files_by_extension(files):\n",
    "    \"\"\"Partition a list of files by their extension.\n",
    "    Args:\n",
    "        files (list): List of file names.\n",
    "    Returns:\n",
    "        tuple: Two lists, one for CSV files and one for XLSX files.\n",
    "    \"\"\"\n",
    "    csv_files = []\n",
    "    xlsx_files = []\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "        elif file.endswith('.xlsx'):\n",
    "            xlsx_files.append(file)\n",
    "    return csv_files, xlsx_files\n",
    "\n",
    "def load_to_dyf(files, glueContext, s3_bucket, logger=None):\n",
    "    \"\"\"Load files into a DynamicFrame.\n",
    "    Args:\n",
    "        files (list): List of file names to load.\n",
    "        glueContext (GlueContext): The Glue context for creating DynamicFrames.\n",
    "        logger (Logger, optional): Logger for logging messages. Defaults to None.\n",
    "    Returns:\n",
    "        list: List of DynamicFrames created from the files.\n",
    "    Raises:\n",
    "        ValueError: If no files are loaded into DynamicFrames.\n",
    "    \"\"\"\n",
    "    dyf_list = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            # print(glueContext,'<--->')\n",
    "            dyf = glueContext.create_dynamic_frame.from_options(\n",
    "                connection_type=\"s3\",\n",
    "                format=\"csv\",\n",
    "                connection_options={\"paths\": [f\"s3://{s3_bucket}/{file}\"]},\n",
    "                format_options={\"withHeader\": True}\n",
    "            )\n",
    "            dyf_list.append(dyf)\n",
    "            logger.info(f\"Loaded file {file} into DynamicFrame\")\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"Error loading file {file}: {e}\")\n",
    "            else:\n",
    "                print(f\"Error loading file {file}: {e}\")\n",
    "\n",
    "    if not dyf_list:\n",
    "        if logger:\n",
    "            logger.error(\"No files were loaded into DynamicFrames.\")\n",
    "        else:\n",
    "            print(\"No files were loaded into DynamicFrames.\")\n",
    "        raise ValueError(\"No files were loaded into DynamicFrames.\")\n",
    "    return dyf_list\n",
    "\n",
    "def consistent_schema(dyf_list):\n",
    "    \"\"\"Check if all DynamicFrames in the list have the same schema.\n",
    "    Args:\n",
    "        dyf_list (list): List of DynamicFrames to check.\n",
    "    Returns:\n",
    "        bool: True if all DynamicFrames have the same schema, False otherwise.\n",
    "    \"\"\"\n",
    "    if not dyf_list:\n",
    "        return True  # Empty list is considered consistent\n",
    "    first_schema = dyf_list[0].schema()\n",
    "    for dyf in dyf_list[1:]:\n",
    "        if dyf.schema() != first_schema:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c79b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python3.11/site-packages/ipykernel_launcher.py', '--f=/root/.local/share/jupyter/runtime/kernel-v337e15d0bef5fdb42276dde43a63b352e99a944d1.json', '--BUCKET_NAME', 'data-bucket-properties-5a166591']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/14 19:26:20 WARN Job$: Job run ID police_data_job is either null or empty or its same as Job name. \n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "BUCKET_NAME = os.getenv('BUCKET_NAME')\n",
    "sys.argv.extend([\"--BUCKET_NAME\",f\"{BUCKET_NAME}\"])\n",
    "print(sys.argv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    params = ['BUCKET_NAME']\n",
    "    if '--JOB_NAME' in sys.argv:\n",
    "        params.append('JOB_NAME')\n",
    "    args = getResolvedOptions(sys.argv, params)\n",
    "\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    glueContext = GlueContext(sc)\n",
    "    spark = glueContext.spark_session\n",
    "    job = Job(glueContext)\n",
    "\n",
    "    if 'JOB_NAME' in args:\n",
    "        jobname = args['JOB_NAME']\n",
    "    else:\n",
    "        jobname = \"police_data_job\"\n",
    "    job.init(jobname, args)\n",
    "\n",
    "    #get logger for this glue job\n",
    "    logger = glueContext.get_logger()\n",
    "    logger.info(f\"Job {jobname} started with args: {args}\")\n",
    "\n",
    "    # Get the S3 bucket name from the arguments\n",
    "    s3_bucket = args['BUCKET_NAME']\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    \n",
    "    # Retrieve csv data files from the S3 bucket\n",
    "    logger.info(f\"Retrieving files from S3 bucket: s3://{s3_bucket}\")\n",
    "    files = get_files_from_s3(s3_bucket, s3_client, logger)\n",
    "    csv_files, xlsx_files = partition_files_by_extension(files)\n",
    "\n",
    "    #load CSV files into DynamicFrames\n",
    "    dyf_list = load_to_dyf(csv_files, glueContext, s3_bucket, logger)\n",
    "    logger.info(f\"Loaded {len(dyf_list)} DynamicFrames from CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efbdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "df = dyf_list[0].toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422f2c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uprn: string (nullable = true)\n",
      " |-- os_topo_toid: string (nullable = true)\n",
      " |-- easting: string (nullable = true)\n",
      " |-- northing: string (nullable = true)\n",
      " |-- postcode_locator: string (nullable = true)\n",
      " |-- administrative_area: string (nullable = true)\n",
      " |-- oa21cd: string (nullable = true)\n",
      " |-- lsoa21cd: string (nullable = true)\n",
      " |-- lsoa21nm: string (nullable = true)\n",
      " |-- lsoa11cd: string (nullable = true)\n",
      " |-- lsoa11nm: string (nullable = true)\n",
      " |-- ward22cd: string (nullable = true)\n",
      " |-- ward22nm: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- built_form: string (nullable = true)\n",
      " |-- property_type_built_form: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- tenure_known: string (nullable = true)\n",
      " |-- building_use: string (nullable = true)\n",
      " |-- construction_age_band: string (nullable = true)\n",
      " |-- construction_age_band_known: string (nullable = true)\n",
      " |-- epc_score: string (nullable = true)\n",
      " |-- epc_score_known: string (nullable = true)\n",
      " |-- epc_rating: string (nullable = true)\n",
      " |-- epc_rating_known: string (nullable = true)\n",
      " |-- potential_epc_score: string (nullable = true)\n",
      " |-- potential_epc_score_known: string (nullable = true)\n",
      " |-- potential_epc_rating: string (nullable = true)\n",
      " |-- potential_epc_rating_known: string (nullable = true)\n",
      " |-- number_habitable_rooms: string (nullable = true)\n",
      " |-- number_habitable_rooms_known: string (nullable = true)\n",
      " |-- total_floor_area: string (nullable = true)\n",
      " |-- total_floor_area_known: string (nullable = true)\n",
      " |-- estimated_floor_count: string (nullable = true)\n",
      " |-- basement_floor: string (nullable = true)\n",
      " |-- wall_type: string (nullable = true)\n",
      " |-- wall_type_known: string (nullable = true)\n",
      " |-- wall_insulation: string (nullable = true)\n",
      " |-- wall_insulation_known: string (nullable = true)\n",
      " |-- roof_type: string (nullable = true)\n",
      " |-- roof_type_known: string (nullable = true)\n",
      " |-- roof_insulation: string (nullable = true)\n",
      " |-- roof_insulation_known: string (nullable = true)\n",
      " |-- glazing_type: string (nullable = true)\n",
      " |-- glazing_type_known: string (nullable = true)\n",
      " |-- main_heat_type: string (nullable = true)\n",
      " |-- main_heat_type_known: string (nullable = true)\n",
      " |-- main_fuel_type: string (nullable = true)\n",
      " |-- main_fuel_type_known: string (nullable = true)\n",
      " |-- energy_consumption: string (nullable = true)\n",
      " |-- energy_consumption_known: string (nullable = true)\n",
      " |-- solar_pv_area: string (nullable = true)\n",
      " |-- solar_pv_potential_11.9: string (nullable = true)\n",
      " |-- avg_tilt: string (nullable = true)\n",
      " |-- imd19_national_decile: string (nullable = true)\n",
      " |-- imd19_income_decile: string (nullable = true)\n",
      " |-- loac_supergroup: string (nullable = true)\n",
      " |-- loac_group: string (nullable = true)\n",
      " |-- fuel_poverty: string (nullable = true)\n",
      " |-- heat_risk_quintile: string (nullable = true)\n",
      " |-- listed_building_grade: string (nullable = true)\n",
      " |-- conservation_area_flag: string (nullable = true)\n",
      " |-- conservation_area_site_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518e4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/14 19:48:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column_name: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for null values in specific column\n",
    "null_count = df.filter(F.col('total_floor_area').isNull()).count()\n",
    "print(f\"Null values in column_name: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-------+--------+----------------+-------------------+------+--------+--------+--------+--------+--------+--------+-------------+----------+------------------------+------+------------+------------+---------------------+---------------------------+---------+---------------+----------+----------------+-------------------+-------------------------+--------------------+--------------------------+----------------------+----------------------------+----------------+----------------------+---------------------+--------------+---------+---------------+---------------+---------------------+---------+---------------+---------------+---------------------+------------+------------------+--------------+--------------------+--------------+--------------------+------------------+------------------------+-------------+-----------------------+--------+---------------------+-------------------+---------------+----------+------------+------------------+---------------------+----------------------+-------------------------+\n",
      "|uprn|os_topo_toid|easting|northing|postcode_locator|administrative_area|oa21cd|lsoa21cd|lsoa21nm|lsoa11cd|lsoa11nm|ward22cd|ward22nm|property_type|built_form|property_type_built_form|tenure|tenure_known|building_use|construction_age_band|construction_age_band_known|epc_score|epc_score_known|epc_rating|epc_rating_known|potential_epc_score|potential_epc_score_known|potential_epc_rating|potential_epc_rating_known|number_habitable_rooms|number_habitable_rooms_known|total_floor_area|total_floor_area_known|estimated_floor_count|basement_floor|wall_type|wall_type_known|wall_insulation|wall_insulation_known|roof_type|roof_type_known|roof_insulation|roof_insulation_known|glazing_type|glazing_type_known|main_heat_type|main_heat_type_known|main_fuel_type|main_fuel_type_known|energy_consumption|energy_consumption_known|solar_pv_area|solar_pv_potential_11.9|avg_tilt|imd19_national_decile|imd19_income_decile|loac_supergroup|loac_group|fuel_poverty|heat_risk_quintile|listed_building_grade|conservation_area_flag|conservation_area_site_id|\n",
      "+----+------------+-------+--------+----------------+-------------------+------+--------+--------+--------+--------+--------+--------+-------------+----------+------------------------+------+------------+------------+---------------------+---------------------------+---------+---------------+----------+----------------+-------------------+-------------------------+--------------------+--------------------------+----------------------+----------------------------+----------------+----------------------+---------------------+--------------+---------+---------------+---------------+---------------------+---------+---------------+---------------+---------------------+------------+------------------+--------------+--------------------+--------------+--------------------+------------------+------------------------+-------------+-----------------------+--------+---------------------+-------------------+---------------+----------+------------+------------------+---------------------+----------------------+-------------------------+\n",
      "+----+------------+-------+--------+----------------+-------------------+------+--------+--------+--------+--------+--------+--------+-------------+----------+------------------------+------+------------+------------+---------------------+---------------------------+---------+---------------+----------+----------------+-------------------+-------------------------+--------------------+--------------------------+----------------------+----------------------------+----------------+----------------------+---------------------+--------------+---------+---------------+---------------+---------------------+---------+---------------+---------------+---------------------+------------+------------------+--------------+--------------------+--------------+--------------------+------------------+------------------------+-------------+-----------------------+--------+---------------------+-------------------+---------------+----------+------------+------------------+---------------------+----------------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for null values in specific column\n",
    "null_count = df.filter(F.col('total_floor_area_known').isNull())\n",
    "null_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+\n",
      "|total_floor_area|total_floor_area_known|\n",
      "+----------------+----------------------+\n",
      "|             138|                     0|\n",
      "|             131|                     0|\n",
      "|             118|                     0|\n",
      "|             147|                     0|\n",
      "|             137|                     0|\n",
      "|             226|                     0|\n",
      "|             129|                     0|\n",
      "|             110|                     0|\n",
      "|             125|                     0|\n",
      "|             126|                     0|\n",
      "|              91|                     0|\n",
      "|             180|                     0|\n",
      "|             118|                     0|\n",
      "|             121|                     0|\n",
      "|             135|                     0|\n",
      "|             126|                     0|\n",
      "|             129|                     0|\n",
      "|             107|                     0|\n",
      "|             108|                     0|\n",
      "|             128|                     0|\n",
      "+----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('total_floor_area', 'total_floor_area_known').filter(F.col('total_floor_area_known') == '0').show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
